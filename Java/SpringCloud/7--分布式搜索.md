# 分布式搜索

## 1. elasticsearch

> 开源搜索引擎，帮助我们从海量数据中快速找到需要的内容，广泛应用在日志分析、实时监控等领域

### 1.1 倒排索引

1. 正向索引：
2. 倒排索引
* 文档(document): 每条数据就是一个文档
* 词条(term): 文档按照语义分成的词语

3. elasticsearch与数据库的关系
* 数据库负责事务类型操作
* elasticsearch负责海量数据的搜索、分析、计算
> 文档数据会被序列化为json格式后存储在elasticsearch中

4. Kibana

5. 分词器
分词器的作用：
* 创建倒排索引时对文档分词
* 用户搜索时，对输入的内容分词

> 处理中文分词，我们一般会使用IK分词器

IK分词器模式：
* ik_smart: 智能切分，粗粒度
* ik_max_word: 最细切分，细粒度

6. 拓展词词典

### 1.2 索引库操作

1. mapping是对索引库中文档的约束，常见的mapping属性包括：
* type 字段数据类型
    * 字符串 ``text(可分词的文本)、keyword(精确值)``
    * 数值 ``long、integer、short、byte、double、float``
    * 布尔 ``boolean``
    * 日期 ``date``
    * 对象 ``object``
* index 是否创建索引，默认为true
* analyzer 使用哪种分词器
* properties 该字段的子字段

2. 查询索引库  ``GET /索引库名``

3. 删除索引库  ``DELETE /索引库名``

4. 修改索引库(添加字段)  ``索引库和mapping一旦创建无法修改，但是可以添加新的字段``
```json
PUT /索引库名/_mapping
{
  "properties": {
    "新字段名": {
      "type": "integer"
    }
  }
}
```

### 1.3 文档操作

1. 新增文档
```json
POST /索引库名/_doc/文档id
{
  "字段1": "值1",
  "字段2": "值2",
  "字段3": {
      "子属性1": "值3",
      "子属性2": "值4",
  }
}
```

2. 查询文档  ``GET /索引库名/_doc/文档id``

3. 删除文档  ``DELETE /索引库名/_doc/文档id``

4. 修改文档
* 全量修改(会删除旧文档，添加新文档)  ``PUT /索引库名/_doc/文档id``
* 增量修改(修改指定字段值)  ``PUT /索引库名/_update/文档id``


### 1.4 RestClient操作索引库

> ES官方提供了各种不同语言的客户端，用来操作ES。这些客户端的本质就是组装DSL语句，通过http请求发送给ES

1. ES中支持两种地理坐标数据类型：
* geo_point 由纬度(latitude)和经度(longitude)确定的一个点
* geo_shape 由多个geo_point组成的复杂几何图形

2. 字段拷贝可以使用copy_to属性将当前字段拷贝到指定字段
```json
"all": {
  "type": "text",
  "analyzer": "ik_max_word"
},
"brand": {
  "type": "keyword",
  "copy_to": "all"
}
```

3. 初始化JavaRestClient
* 引入es的RestHighLevelClient依赖
```xml
<dependency>
  <groupId>org.elasticsearch.client</groupId>
  <artifactId>elasticsearch-rest-high-level-client</artifactId>
</dependency> 

<!--因为Springboot默认的ES版本是7.6.2，所以我们需要覆盖默认的ES版本-->
<properties>
    <java.version>1.8</java.version>
    <elastiicsearch.version>7.12.1</elastiicsearch.version>
</properties>
```
* 初始化RestHighLevelClient
```java
public class HotelIndexTest {
  private RestHignLevelClient client;

  @BeforeEach
  void setUp() {
    this.client = new RestHignLevelClient(RestClient.builder(
            HttpHost.create("http://192.168.133.128:9200"),
            HttpHost.create("http://192.168.133.128:9200"),
            ));
  }

  @AfterEach
  void tearDown() throws IOException {
      this.client.close();
  }
}
```
4. 利用RestHighLevelClient创建索引库
```java
@Test
void testCreateHotelIndex() throws IOException{
    // 1. 创建Request对象
    CreateIndexRequest request = new CreateIndexRequest("hotel");
    // 2. 请求参数，MAPPING_TEMPLATE是静态常量字符串，内容是创建索引库的DSL语句
    request.source(MAPPING_TEMPLATE,XcontentType.JSON);
    // 3. 发起请求
    client.indices().create(request, RequestOptions.DEFAULT);    
        }
```

5. 利用RestHighLevelClient删除索引库
6. 利用RestHighLevelClient判断索引库是否存在

### 1.5 RestClient操作文档

1. 新增文档
2. 查询文档
3. 更新文档
4. 删除文档
5. 批量写入数据
```java
@SpringBootTest
class HotelDocumentTest {

    private RestHighLevelClient client;

    @Autowired
    private IHotelService hotelService;

    @Test
    void testAddDocument() throws IOException {
        // 1.查询数据库hotel数据
        Hotel hotel = hotelService.getById(61083L);
        // 2.转换为HotelDoc
        HotelDoc hotelDoc = new HotelDoc(hotel);
        // 3.转JSON
        String json = JSON.toJSONString(hotelDoc);

        // 1.准备Request
        IndexRequest request = new IndexRequest("hotel").id(hotelDoc.getId().toString());
        // 2.准备请求参数DSL，其实就是文档的JSON字符串
        request.source(json, XContentType.JSON);
        // 3.发送请求
        client.index(request, RequestOptions.DEFAULT);
    }

    @Test
    void testGetDocumentById() throws IOException {
        // 1.准备Request      // GET /hotel/_doc/{id}
        GetRequest request = new GetRequest("hotel", "61083");
        // 2.发送请求
        GetResponse response = client.get(request, RequestOptions.DEFAULT);
        // 3.解析响应结果
        String json = response.getSourceAsString();

        HotelDoc hotelDoc = JSON.parseObject(json, HotelDoc.class);
        System.out.println("hotelDoc = " + hotelDoc);
    }

    @Test
    void testDeleteDocumentById() throws IOException {
        // 1.准备Request      // DELETE /hotel/_doc/{id}
        DeleteRequest request = new DeleteRequest("hotel", "61083");
        // 2.发送请求
        client.delete(request, RequestOptions.DEFAULT);
    }

    @Test
    void testUpdateById() throws IOException {
        // 1.准备Request
        UpdateRequest request = new UpdateRequest("hotel", "61083");
        // 2.准备参数
        request.doc(
                "price", "870"
        );
        // 3.发送请求
        client.update(request, RequestOptions.DEFAULT);
    }

    @Test
    void testBulkRequest() throws IOException {
        // 查询所有的酒店数据
        List<Hotel> list = hotelService.list();
        // 1.准备Request
        BulkRequest request = new BulkRequest();
        // 2.准备参数
        for (Hotel hotel : list) {
            // 2.1.转为HotelDoc
            HotelDoc hotelDoc = new HotelDoc(hotel);
            // 2.2.转json
            String json = JSON.toJSONString(hotelDoc);
            // 2.3.添加请求
            request.add(new IndexRequest("hotel").id(hotel.getId().toString()).source(json, XContentType.JSON));
        }
        // 3.发送请求
        client.bulk(request, RequestOptions.DEFAULT);
    }

    @BeforeEach
    void setUp() {
        client = new RestHighLevelClient(RestClient.builder(
                HttpHost.create("http://192.168.133.128:9200")
        ));
    }

    @AfterEach
    void tearDown() throws IOException {
        client.close();
    }
}
```

## 2. 分布式搜索引擎

### 2.1 DSL的查询语法
```
GET /indexName/_search
{
  "query": {
    "查询类型": {
      "查询条件": "条件值"
    }
  }
}
```

1. 常见的查询类型包括：
* 查询所有  ``match_all``
```json
GET /indexName/_search
{
  "query": {
    "match_all": {}
  }
}
```
* 全文检索  ``即倒排索引,match_query,multi_match_query``
```json
// match查询：全文检索的一种，会对用户输入内容分词，然后去倒排索引库检索
GET /indexName/_search
{
  "query": {
    "match": {
      "FILED": "TEXT"
    }
  }
}
```
```json
// multi_match：与match查询类似，只不过允许同时查询多个字段(不推荐，参与查询字段越多，查询性能越差)
GET /indexName/_search
{
  "query": {
    "multi_match": {
      "query": "TEXT",
      "fields": ["FIELD1", "FIELD2"]
    }
  }
}
```
* 精确查询  ``ids、range、term``
```json
// term: 根据词条精确值查询
GET /indexName/_search
{
  "query": {
    "term": {
      "FIELD": {
        "value": "VALUE"
      }
    }
  }
}
```
```json
// range: 根据值的范围查询
GET /indexName/_search
{
 "query": {
    "range": {
      "FIELD": {
        "gte": 10,  # 大于等于  gt: 大于
        "lte": 20   # 小于等于  lt: 小于
      }
    }
  }
}
```
* 地理查询  ``geo_distance、geo_bounding_box``
```json
// geo_bounding_box: 查询geo_point值落在某个矩形范围的所有文档
GET /indexName/_search
{
 "query": {
    "geo_bounding_box": {
        "FIELD": {
          "top_left": {
             "lat": 31.1,
             "lon": 121.5
          }
          "bottom_right": {
             "lat": 30.9,
             "lon": 121.7
          }
        }
    }
  }
}
```
```json
// geo_distance: 查询到指定中心点小于某个距离值的所有文档
GET /indexName/_search
{
 "query": {
    "geo_distance": {
      "distance": "15km",
      "FIELD": "31.21,121.5"
    }
  }
}
```
* 复合(compound)查询  ``bool、function_score``
  * function_score: 算分函数查询，可以控制文档相关性算分，控制文档排名
  * TF、TF-IDF、BM25算法
```json
// function score query: 可以修改文档的相关性算分
GET /indexName/_search
{
 "query": {
    "function_score": {
      "query": { "match": {"all": "外滩"} },  // 原始查询条件，搜索文档并根据相关性打分(query score)
      "functions": [
        {
          "filter": {"term": {"id": "1"}},  // 过滤条件，符合条件的文档才会被重新算分
          "weight": 10  // 算分函数，算分函数的结果称为function score，将来会与query score运算，得到一个新算分，weight、fileld_value_factor、random_score、script_score
        }
      ],
      "boost_mode": "multiply"  // 加权模式，定义function score与query score的运算模式，包括multiply、replace、sum、avg、max等
    }
  }
}
```
```json
// Boolean Query: 布尔查询是一个或多个查询子句的组合，must、should、must_not、filter(必须匹配，不参与算分)
GET /indexName/_search
{
  "query": {
    "bool": {
      "must": [
        {
            "term": {"city": "上海"}
        }
      ],
      "should": [
        {
          "term": {"brand": "皇冠假日"}
        },
          {
        "term": {"brand": "华美达"}
        }
      ],
      "must_not": [
        {
          "range": {"price": {"lte": 500}}
        }
      ],
      "filter": [
        {
          "range": {
            "score":{"gte": 45}
            }
        }
      ]
    }
  }
}
```

### 2.2 搜索结果处理

1. 排序
```json
GET /indexName/_search
{
  "query": {
    "match_all": {}
  },
  "sort": [
    {
      "FIELD": "desc"   // 排序字段和排序方式ASC、DESC
    }
  ]
}
```

2. 分页(from+size、after search、scroll)
```json
GET /indexName/_search
{
  "query": {
    "match_all": {}
  },
  "from": 990,  // 分页开始的位置，默认为0
  "size": 10,  // 期望获取的文档总数
  "sort": [
    {
      "FIELD": "desc"   
    }
  ]
}
```
> 深度分页问题：ES是分布式的，ES集群，ES设定结果集查询的上限是10000

3. 高亮
> 默认情况下，ES搜索字段必须与高亮字段一致，不要一致可以指定require_field_match = "false"
```json
GET /indexName/_search
{
  "query": {
    "match": {
      "FIELD": "TEXT"
    }
  },
  "hignLight": {
    "fields": {  // 指定要高亮的字段
      "FIELD": {
        "pre_tags": "<em>",  // 用来标记高亮字段的前置标签
        "post_tags": "</em>"  // 用来标记高亮字段的后置标签
      }
    }
  }
}
```

## 3. RestClient查询文档

1. 查询的基本步骤
* 创建SearchRequest对象
* 准备Request.source()，也就是DSL
  * QueryBuilders来构建查询条件
  * 传入Request.source()的query()方法
* 发送请求，得到结果
* 解析结果
```java
public class HotelSearchTest {
    private RestHighLeevelClient client;
    
    @Test
    void testMatchAll() throws IOException {
      // 1. 准备Request
      SearchRequest request = new SearchRequest("hotel");
      // 2. 准备DSL
      request.source().query(QueryBuilders.matchAllQuery());
      // 3. 发送请求
      SearchResponse response = client.search(request, RequestOptions.DEFAULT);
      
      // 4. 解析响应
      SearchHits searchHits = response.getHits();
      // 4.1 获取总条数
      long total = searchHits.getTotalHits().values;
      System.out.println("共搜到"+total+"条数据");
      // 4.2 文档数组
      SearchHit[] hits = searchHits.getHits();
      // 4.3 遍历
      for(SearchHit hit: hits){
        // 获取文档source
        String json = hit.getSourceAsString();
        // 反序列化
        HotelDoc hotelDoc = JSON.parseObject(json,HotelDoc.class);
        System.out.println("hotelDoc="+hotelDoc);
      }
    }
} 
```

2. 全文检索查询

全文检索的match、multi_match查询和match_all的API基本一致。差别是查询条件，也就是query的部分
> 单字段查询 QueryBuilders.matchQuery("all","如家");
> 
> 多字段查询 QueryBuilders.multiMatchQuery("如家","name","business");

3. 精确查询

> 词条查询 QueryBuilders.termQuery("city","杭州");
> 
> 范围查询 QueryBuilders.rangeQuery("price").gte(100).lte(150);

4. 复合查询

> 创建布尔查询 BoolQueryBuilder boolQuery = QueryBuilders.boolQuery();
> 
> 添加must条件 boolQuery.must(QueryBuilders.termQuery("city","杭州"));
> 
> 添加filter条件 boolQuery.filter(QueryBuilders.rangeQuery("price").lte(250));

5. 排序和分页

> 查询 request.source().query(QueryBuilders.matchAllQuery());
> 
> 分页 request.source().from(0).size(5);
> 
> 价格排序 request.source().sort("price", SortOrder.ASC);

```java
public class HotelSearchTest {
  private RestHighLeevelClient client;

  @Test
  void testPageSort() throws IOException {
    // 页码，每页大小
    int page = 1, size = 5;
    // 1. 准备Request
    SearchRequest request = new SearchRequest("hotel");
    // 2. 准备DSL
    // 2.1 query
    request.source().query(QueryBuilders.matchAllQuery());
    // 2.2 排序
    request.source().sort("price", SortOrder.ASC);
    // 2.3 分页
    request.source().from((page-1)*size).size(5);
    // 3. 发送请求
    SearchResponse response = client.search(request, RequestOptions.DEFAULT);

    // 4. 解析响应
    SearchHits searchHits = response.getHits();
    // 4.1 获取总条数
    long total = searchHits.getTotalHits().values;
    System.out.println("共搜到" + total + "条数据");
    // 4.2 文档数组
    SearchHit[] hits = searchHits.getHits();
    // 4.3 遍历
    for (SearchHit hit : hits) {
      // 获取文档source
      String json = hit.getSourceAsString();
      // 反序列化
      HotelDoc hotelDoc = JSON.parseObject(json, HotelDoc.class);
      System.out.println("hotelDoc=" + hotelDoc);
    }
  }
} 
```

6. 高亮

> request.source().highlighter(new HighlightBuilder()
> .field("name")
> .requireFieldMatch(false)
> );

```java
public class HotelSearchTest {
    private RestHighLeevelClient client;
    
    @Test
    void testHighlight() throws IOException {
      // 1. 准备Request
      SearchRequest request = new SearchRequest("hotel");
      // 2. 准备DSL
      // 2.1 query
      request.source().query(QueryBuilders.matchQuery("all","如家"));
      // 2.2 高亮
      request.source().highlighter(new HighlightBuilder().field("name").requireFieldMatch(false));
      // 3. 发送请求
      SearchResponse response = client.search(request, RequestOptions.DEFAULT);
      // 4. 解析响应
      SearchHits searchHits = response.getHits();
      // 4.1 获取总条数
      long total = searchHits.getTotalHits().values;
      System.out.println("共搜到"+total+"条数据");
      // 4.2 文档数组
      SearchHit[] hits = searchHits.getHits();
      // 4.3 遍历
      for(SearchHit hit: hits){
        // 获取文档source
        String json = hit.getSourceAsString();
        // 反序列化
        HotelDoc hotelDoc = JSON.parseObject(json,HotelDoc.class);
        // 获取高亮结果
        Map<String, HighlightField> highlightFields =hit.getHighlightFields();
        if(!CollectionUtils.isEmpty(highlightFields)){
            // 获取高亮字段
          HighlightField highlightField = highlightFields.get("name");
          if(highlightField!=null){
              // 取出高亮结果数组中的第一个，就是酒店名称
            String name = highlightField.getFragments()[0].string();
            hotelDoc.setName(name);
          }
        }
        System.out.println("hotelDoc="+hotelDoc);
      }
    }
} 
```

## 4. 数据聚合

1. 聚合可以实现对文档数据的统计、分析、运算。聚合常见的有三类：
* 桶(Bucket)聚合  ``用来对文档进行分组``
  * TermAggregation  按照文档字段值分组
```json
// 默认情况下，Bucket聚合会统计Bucket内的文档数量，记为_count，并且按照_count降序排序
// 默认情况下，Bucket聚合是对索引库的所有文档做聚合，我们可以先定要聚合的文档范围，只要添加query条件即可
GET /hotel/_search
{
  "query": {
    "range": {
      "price": {
        "lte": 200
      }
    }
  },
  "size": 0, // 设置size为0，结果中不包含文档，只包含聚合结果
  "aggs": { // 定义聚合
    "brandAgg": {  // 给聚合起个名字
      "terms": {  // 聚合的类型，按照品牌值聚合，所以选择term
        "field": "brand",  // 参与聚合的字段
        "order": {
          "_count": "desc"
        },
        "size": 20  // 希望获取的聚合结果数量
      }
    }
  }
}
```
  * Date Histogram 按照日期阶梯分组

* 度量(Metric)聚合  ``用以计算一些值``
  * Avg
  * Max
  * Min
  * Stats：同时求max、min、avg、sum
```json
GET /hotel/_search
{
  "size": 0,
  "aggs": {
    "brandAgg": {
      "terms": {
        "field": "brand",
        "size": 20,
        "order": {
          "scoreAgg.avg": "desc"  //对结果的自定义排序
        }
      },
      "aggs": {  // 是brands聚合的子聚合，也就是分组后对每组分别计算
        "score_stats": {  // 聚合名称
          "stats": {  // 聚合类型，这里stats可以计算min、max、avg等
            "field": "score" // 聚合字段，这里是score
          }
        }
      }
    }
  }
}
```
* 管道(pipeline)聚合  ``其他聚合的结果为基础聚合``

2. RestClient实现聚合
```java
public class HotelSearchTest {
  private RestHighLeevelClient client;
  
  @Test
  void testAggregation() throws IOException{
    // 1. 准备Request
    SearchRequest request = new SearchRequest("hotel");
    // 2. 准备DSL
    // 2.1 设置size
    request.source().size(0);
    // 2.2 聚合
    request.source().aggregation(AggregationBuilders
            .terms("brandAgg")
            .field("brand")
            .size(10)
    );
    // 3. 发出请求
    SearchResponse response = client.search(request, RequestOptions.DEFAULT);
    // 4. 解析结果
    Aggregations aggregations = response.getAggregations();
    // 4.1 根据聚合名称获取聚合结果
    Terms brandTerms = aggregations.get("brandAge");
    // 4.2 获取buckets
    List<? extends Terms.Bucket> buckets = brandTerms.getBuckets();
    // 4.3 遍历
    for(Terms.Bucket bucket:buckets){
      // 4.4 获取key
      String key = bucket.getKeyAsString();
      System.out.println(key);
    }
  }
}
```
5. 多条件聚合

6. 带过滤条件的聚合

* 对接前端接口
  * 编写controller接口，接收该请求
  * 修改IUserService#getFilters()方法，添加RequestParam参数
  * 修改getFilters方法的业务，聚合时添加query条件

## 5. 自动补全

### 5.1 分词器
1. elasticsearch中分词器(analyzer)的组成包含三部分：
* character filters: 在tokenizer之前对文本进行处理。例如删除字符、替换字符
* tokenizer: 将文本按照一定的规则切割成词条(term)。例如keyword，就是不分词；还有ik_smart
* tokenizer: 将tokenizer输出的词条做进一步处理。例如大小写转换、同义词处理、拼音处理等

2. 拼音分词器

3. 自定义分词器
> 我们可以在创建索引库时，通过settings来配置自定义的analyzer(分词器)
>
> 创建索引库与搜索索引库应使用不同的分词器
```json
PUT /test
{
  "settings": {  
    "analysis": {
      "analyzer": {  // 自定义分词器
        "my_analyzer": {  // 分词器名称
          "tokenizer": "ik_max_word",  // 先分好词
          "filter": "py"  // 然后再交给pinyin处理
        }
      },
      "filter": {
        "py": {
          "type": "pinyin",
          "keep_full_pinyin": false,
          "keep_joined_full_pinyin": true,
          "keep_original": true,
          "limit_first_letter_length": 16,
          "remove_duplicated_term": true,
          "none_chinese_pinyin_tokenize": false
        }
      },
      "mappings": {
        "properties": {
          "name": {
            "type": "text",
            "analyzer": "my_analyzer",
            "search_analyzer": "ik_smart"
          }
        }
      }
    }
  }
}
```

### 5.2 DSL实现自动补全查询
1. completion suggest查询：为了提高补全查询的效率，对于文档中字段的类型有一些约束
* 参与补全查询的字段必须是completion类型
* 字段的内容一般是用来补全的多个词条形成的数组
```json
// 查询语法
GET /test/_search
{
  "suggest": {
    "title_suggest": {
      "text": "s",  // 关键字
      "completion": {
        "field": "title",  // 补全查询的字段
        "skip_duplicates": true,  // 跳过重复的
        "size": 10  // 获取前10条结果
      }
    }
  }
}
```

2. RestAPI实现自动补全
```java
public class HotelSearchTest {
  private RestHighLeevelClient client;
  @Test
  void testSuggest() throws IoException {
    // 1. 准备Request
    SearchRequest request = new SearchRequest("hotel");
    // 2. 准备DSL
    request.source().suggest(new SuggestBuilder().addSuggestion(
            "suggestions",
            SuggestBuilders.completionSuggestion("suggestion")
                    .prefix("h")
                    .skipDuplicates(true)
                    .size(10)
    ));
    // 3. 发起请求
    SearchResponse response = client.search(request, RequestOptions.DEFAULT);
    // 4. 解析结果
    Suggest suggest = response.getSuggest();
    // 4.1 根据补全查询名称，获取补全结果
    CompletionSuggestion suggestions = suggest.getSuggestion("suggestions");
    // 4.2 获取options
    List<CompletionSuggestion.Entry.Option> options = suggestions.getOptions();
    // 4.3 遍历
    for(CompletionSuggestion.Entry.Option option : options){
        String text = option.getText().toString();
      System.out.println(text);
    }
  }
}
```

## 6. 数据同步

> elasticsearch与mysql之间的数据同步

1. 同步调用  ``缺点：耦合度高``
* 写入数据库
* 调用hotel-demo的更新索引库接口
* 自己调用接口来更新elasticsearch

2. 异步通知  ``缺点：依赖mq的可靠性``
* 写入数据库
* 发布消息(MQ)
* 监听消息
* 更新elasticsearch

3. 监听binlog  ``缺点：开启binlog增加数据库负担、实现复杂度高``
* 写入数据库
* 监听mysql的binlog(canal)
* 通知酒店数据变更情况
* 更新elasticsearch

## 7. ES集群

1. 单机的elasticsearch做数据存储，必然面临两个问题：海量数据存储问题、单点故障问题
* 海量存储问题：将索引库从逻辑上拆分为N个分片(shard)，存储到多个节点
* 单点故障问题：将分片数据在不同节点备份(replica)

2. ES集群的节点角色

| 节点类型       | 配置参数 | 默认值 | 节点职责 | 
|------------| ------ | ------- | -----|
 master eligible | node.master | true | 备选主节点
 data       | node.data   | true | 数据节点：CRUD
 ingest     | node.ingest | true | 数据存储之前的预处理
 coordinating | 上面三个参数都为false时则为coordinating节点 | 无 | 路由请求到其他节点，合并其他节点处理的结果，返回给用户

3. ES集群的脑裂

> 默认情况下，每个节点都是master eligible节点，因此一旦master节点宕机，其他候选节点会选举一个成为主节点。当主节点与其他节点网络故障时，可能发生脑裂问题
> 
> 为了避免脑裂，需要要求选票超过(eligible节点数量 + 1)/2才能当选为主，因此eligible节点数量最好是奇数

4. ES集群的分布式存储

> 新增文档时，elasticsearch会通过hash算法来计算文档应该存储到哪个分片： shard = hash(_routing) % number_of_shards

* _routing默认是文档的id
* 算法与分片数量有关，因此索引库一旦创建，分片数量不能修改

5. ES集群的分布式查询
分两个阶段：
* scatter phase 分散阶段 coordinating node会把请求分发到每一个分片
* gather phase 聚集阶段 coordinating node汇总data node的搜索结果，并处理为最终结果集返回给用户

6. ES集群的故障转移
* master宕机后，EligibleMaster选举为新的主节点
* master节点监控分片、节点状态，将故障节点上的分片转移到正常节点，确保数据安全

